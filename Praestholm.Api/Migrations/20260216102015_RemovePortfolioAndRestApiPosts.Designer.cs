// <auto-generated />
using System;
using Microsoft.EntityFrameworkCore;
using Microsoft.EntityFrameworkCore.Infrastructure;
using Microsoft.EntityFrameworkCore.Metadata;
using Microsoft.EntityFrameworkCore.Migrations;
using Microsoft.EntityFrameworkCore.Storage.ValueConversion;
using Praestholm.Api.Data;

#nullable disable

namespace Praestholm.Api.Migrations
{
    [DbContext(typeof(BlogDbContext))]
    [Migration("20260216102015_RemovePortfolioAndRestApiPosts")]
    partial class RemovePortfolioAndRestApiPosts
    {
        /// <inheritdoc />
        protected override void BuildTargetModel(ModelBuilder modelBuilder)
        {
#pragma warning disable 612, 618
            modelBuilder
                .HasAnnotation("ProductVersion", "8.0.24")
                .HasAnnotation("Relational:MaxIdentifierLength", 128);

            SqlServerModelBuilderExtensions.UseIdentityColumns(modelBuilder);

            modelBuilder.Entity("Praestholm.Api.Models.BlogPost", b =>
                {
                    b.Property<int>("Id")
                        .ValueGeneratedOnAdd()
                        .HasColumnType("int");

                    SqlServerPropertyBuilderExtensions.UseIdentityColumn(b.Property<int>("Id"));

                    b.Property<string>("Author")
                        .IsRequired()
                        .HasMaxLength(100)
                        .HasColumnType("nvarchar(100)");

                    b.Property<string>("Content")
                        .IsRequired()
                        .HasColumnType("nvarchar(max)");

                    b.Property<DateTime>("CreatedAt")
                        .HasColumnType("datetime2");

                    b.Property<string>("Description")
                        .IsRequired()
                        .HasMaxLength(500)
                        .HasColumnType("nvarchar(500)");

                    b.Property<bool>("IsPublished")
                        .HasColumnType("bit");

                    b.Property<DateTime?>("PublishedAt")
                        .HasColumnType("datetime2");

                    b.Property<string>("Slug")
                        .IsRequired()
                        .HasMaxLength(200)
                        .HasColumnType("nvarchar(200)");

                    b.Property<string>("Tags")
                        .IsRequired()
                        .HasMaxLength(500)
                        .HasColumnType("nvarchar(500)");

                    b.Property<string>("Title")
                        .IsRequired()
                        .HasMaxLength(200)
                        .HasColumnType("nvarchar(200)");

                    b.Property<DateTime>("UpdatedAt")
                        .HasColumnType("datetime2");

                    b.HasKey("Id");

                    b.HasIndex("Slug")
                        .IsUnique();

                    b.ToTable("BlogPosts");

                    b.HasData(
                        new
                        {
                            Id = 4,
                            Author = "Oskar Praestholm",
                            Content = "# My Master's Thesis: Query-Based Subscription for IoT Data Streams\n\nIn June 2025 I completed my master's thesis in Software Engineering at the University of Southern Denmark (SDU), supervised by Aslak Johansen. The thesis investigates whether a **query-based subscription (QBS) model** can provide a more scalable and flexible alternative to the traditional query-loop approach for managing IoT data streams.\n\n## The Problem\n\nIn dynamic IoT environments, new devices and data streams can appear at any time. Traditional systems rely on a **query-loop** approach where clients continuously poll for new data sources. Every time a new sensor is added, the query must be updated to include it. This leads to inefficiencies, increased development complexity, and a constant need for manual intervention as the system grows.\n\nThe thesis asks: *what if the system could automatically push relevant data streams to clients as they appear, based on declarative subscriptions?*\n\n## The Approach\n\nThe proposed solution flips the model from **pull to push**. Instead of clients repeatedly querying for new sources, they subscribe once using metadata-driven queries, and the system automatically routes matching data streams as they become available.\n\n### Key Technologies\n\n- **MQTT** for real-time data exchange between the broker and clients\n- **Neo4j** graph database for storing and querying IoT metadata (devices, rooms, sensor types, and their relationships)\n- **Akka.NET** actor framework for building a modular, concurrent custom broker\n- **Cypher** queries for defining flexible, graph-based subscriptions\n\n## System Architecture\n\nThe system is built around a **custom MQTT broker** structured using an actor-based model. Each actor handles a specific responsibility:\n\n- **PackageListener** receives incoming MQTT packets and routes them to the appropriate handler\n- **PublishHandler** extracts payloads and forwards them to the MessageRouter\n- **SubscribeHandler** evaluates whether a subscription targets a static topic or a dynamic virtual topic\n- **VirtualTopicValidator** manages metadata-aware subscriptions by matching new data streams against precomputed label sets\n- **MessageRouter** delivers data to all matching subscribers\n- **EventNotifier** pushes real-time updates via WebSocket\n\n### The SmartFilter Mechanism\n\nA core innovation is the **SmartFilter**. Instead of evaluating the full Cypher query every time new data arrives, the system runs the query once at subscription time and extracts a set of *expected labels* (e.g., sensor type = \"temperature\", room = \"Room-A\"). New data streams are then matched against these labels at runtime, avoiding repeated database queries.\n\nThis shifts the cost from runtime to subscription time, making message routing fast and lightweight while still supporting expressive metadata-based filtering.\n\n### Graph-Based Metadata Model\n\nThe metadata is stored as a **labeled property graph** in Neo4j. IoT devices are nodes linked to their data streams, rooms, buildings, and sensor types. This allows users to write a single Cypher query like *\"give me all temperature streams in Building 42\"* and have the system automatically create a virtual topic that follows matching data streams, including ones added in the future.\n\n## Evaluation and Results\n\nThe QBS model was compared against the traditional query-loop (QL) approach across latency, memory usage, and CPU behavior with 10 to 4,000 concurrent queries.\n\n**Key findings:**\n\n- **Stable latency:** QBS averaged ~2 seconds end-to-end latency, consistent across all workloads, with no added overhead once SmartFilters were registered\n- **CPU behavior:** QBS showed event-driven CPU load variance, while QL maintained steady polling cycles with uniform resource consumption\n- **Memory trade-off:** QBS uses more memory upfront for SmartFilter construction (~167-171 MB) but stabilizes, while QL starts lighter (~2.5 MB) but grows less predictably with active queries\n- **Best suited for:** Systems with long-lived subscriptions and high message throughput per topic\n\nWhile the results were not conclusive enough to declare QBS definitively superior, the system demonstrates **technical feasibility** and shows clear architectural benefits for certain workload profiles.\n\n## Lessons Learned\n\n1. **Protocol compliance matters** — Early reliance on the permissive Mosquitto test client masked missing MQTT features (like keep-alive handling) that only surfaced with stricter clients later\n2. **Actor boundaries need discipline** — The modular actor design provided clarity, but components like the MessageRouter accumulated too many responsibilities under load\n3. **SmartFilters trade flexibility for performance** — The approach works well when subscriptions are stable, but struggles when they change frequently\n4. **Custom brokers are powerful but demanding** — Full control over metadata, queries, and subscriptions came at the cost of development complexity and ecosystem maturity\n\n## Looking Forward\n\nFuture work includes revisiting the broker architecture to reduce latency, adding proper MQTT keep-alive support, introducing load balancing, and conducting evaluations in isolated test environments at larger scale.\n\nThe thesis establishes a foundation for **adaptive, declarative IoT data integration** — a step toward systems where developers describe what data they need, and the infrastructure handles the rest.",
                            CreatedAt = new DateTime(2025, 2, 14, 12, 0, 0, 0, DateTimeKind.Utc),
                            Description = "A summary of my master's thesis on designing a query-based subscription model for managing IoT data streams, featuring a custom MQTT broker, Neo4j metadata graph, and SmartFilter mechanism.",
                            IsPublished = true,
                            PublishedAt = new DateTime(2025, 2, 14, 12, 0, 0, 0, DateTimeKind.Utc),
                            Slug = "masters-thesis-query-based-subscription-iot",
                            Tags = "IoT,MQTT,Thesis,Software Engineering,.NET",
                            Title = "My Master's Thesis: Query-Based Subscription for IoT Data Streams",
                            UpdatedAt = new DateTime(2025, 2, 14, 12, 0, 0, 0, DateTimeKind.Utc)
                        },
                        new
                        {
                            Id = 5,
                            Author = "Oskar Praestholm",
                            Content = "# My Bachelor's Thesis: Scaling and Resource-Optimization with Docker\n\nIn June 2023 I completed my bachelor's project in Software Engineering at the University of Southern Denmark (SDU), together with Jeppe Stenstrup Lauridsen. The project was done in collaboration with Vitec Aloc and focused on modernizing their internal Booking system by replacing virtual machines with Docker containers.\n\n## The Problem\n\nVitec Aloc is a software company that develops investment management solutions. Their developers rely on an internal **Booking system** to spin up isolated test environments where they can install and test their product, PORTMAN, against customer configurations.\n\nThe existing system was built on **SCVMM** (System Center Virtual Machine Manager), which managed full Windows virtual machines. Each VM came with a complete Windows installation and a preset resource allocation — whether the developer needed all those resources or not. This led to several problems:\n\n- **Wasted resources** — VMs consumed around 40 GB of disk space each for a full Windows installation, and developers would over-allocate memory to avoid build failures\n- **Frequent downtime** — The SCVMM database would fall out of sync every ~3 weeks, requiring a full physical restart of the server\n- **High maintenance cost** — The Booking Server was built on old Java code that nobody wanted to touch, with Jenkins jobs patched in as workarounds for recurring failures\n- **No resilience** — When something broke, it stayed broken until someone manually intervened\n\nThe thesis asked: *can we scale and optimize the resource usage in Vitec's Booking system with Docker, and further make it maintainable and resilient towards failure?*\n\n## The Approach\n\nWe proposed replacing the VM-based system with **Docker containers** running on Windows Server Core. Instead of provisioning an entire virtual machine for each test slot, we would build lightweight Docker Images containing only the PORTMAN installer, and spin up containers on demand.\n\n### Key Technologies\n\n- **Docker** for containerization on Windows Server Core\n- **ASP.NET Core** with Blazor for a full-stack C# solution\n- **Entity Framework Core** with MSSQL for data persistence\n- **MediatR** for implementing the CQRS pattern (commands and queries)\n- **Clean Architecture** for maintainable, layered code structure\n- **MudBlazor** for the admin dashboard UI\n- **NUnit** and **FluentAssertions** for testing\n\n## System Architecture\n\nThe system was designed around a central **Booking Server** that communicates with the Docker Daemon, a database, a fileserver, and the GUI. The server handles the full container lifecycle: downloading installation files, building Docker Images, creating and managing containers.\n\n### The Pipeline\n\nA key challenge was building Docker Images dynamically. The PORTMAN installation file needed to be copied into the image, but network policies prevented Docker from accessing the company fileserver directly during builds. We solved this by downloading the installer locally first, then using Docker build arguments to inject it into a generic Dockerfile:\n\nThe Dockerfile was kept minimal — base it on Windows Server Core, set the shell to PowerShell, copy the installer, and run it with the target database as a parameter. This allowed every PORTMAN version and branch to share the same Dockerfile template.\n\n### Clean Architecture and CQRS\n\nMidway through the project, Vitec Aloc requested we adopt **Clean Architecture** to ensure long-term maintainability. This restructured the codebase into distinct layers (Domain, Application, Infrastructure, Presentation) with clear dependency rules.\n\nWe used **MediatR** to implement the CQRS pattern, separating commands (create container, start container, build image) from queries (get container info, list all containers). Each operation became a self-contained handler with its dependencies injected, making the code modular and testable.\n\n### Availability Tactics\n\nTo address the resilience problems of the old system, we implemented **health checks** using ASP.NET Core's built-in health check framework:\n\n- **MSSQL Database check** — Verifies the database is reachable\n- **Fileserver check** — Ensures the installation file server is accessible\n- **Docker Daemon check** — Confirms the Docker client can communicate with the daemon\n- **Database/Docker sync check** — Compares container statuses in the database against actual Docker Daemon state, flagging mismatches\n\nIf any dependency was unavailable, the system would gracefully limit operations rather than crash. An admin dashboard displayed all health check results with a polling interval, giving maintainers a real-time overview of system health.\n\n## Development Process\n\nWe worked in **two iterations**. The first was a proof of concept: get Docker running with Windows containers, prove we could create, start, pause, stop, and remove containers programmatically through C#. The second iteration built the full Booking system with Clean Architecture, the database layer, the Blazor GUI, and availability tactics.\n\nThe mid-project switch to Clean Architecture was a significant pivot that cost us time, but it was the right decision for the codebase's future. It also taught us an important lesson about stakeholder communication — had we confirmed the architectural requirements earlier, we could have avoided the rework.\n\n## Results\n\n**Key findings:**\n\n- **93.75% disk reduction** — Docker containers using Windows Server Core consumed only ~2.5 GB compared to ~40 GB for full VM installations\n- **Improved maintainability** — Clean Architecture and the CQRS pattern made the codebase modular and approachable for Vitec Aloc's own developers\n- **Better resilience** — Health checks provided continuous monitoring and prevented the system from operating on a faulty foundation\n- **Pausable containers** — Unlike VMs that always consumed resources, Docker containers could be paused over weekends without constant resource usage\n\nScalability was scoped out due to time constraints — the system is stateful (tied to a single Docker Daemon), and true horizontal scaling would require an orchestrator like Kubernetes. This remains as future work.\n\n## Lessons Learned\n\n1. **Clarify architecture early** — The late switch to Clean Architecture was valuable but costly. Confirming technical requirements with stakeholders upfront saves time\n2. **Docker on Windows has quirks** — The Docker.DotNet library couldn't build images the way we needed, so we had to shell out to `docker build` via `System.Diagnostics.Process`\n3. **Network policies matter** — Corporate network restrictions blocked our initial Dockerfile approach, forcing a workaround with local file downloads\n4. **Test what you build** — We missed testing the image build functionality in our first iteration, which was the one feature that couldn't use the Docker.DotNet library. Earlier testing would have caught this gap sooner\n5. **Containers are not VMs** — The shift from virtual machines to containers required rethinking resource allocation, lifecycle management, and how installation processes work\n\n## Looking Forward\n\nFuture work includes adding Kubernetes for horizontal scalability, implementing logging for real-time debugging, expanding test coverage, and adding support for all of Vitec Aloc's products beyond PORTMAN. With those additions, the system could serve as a full replacement for the existing Booking server.",
                            CreatedAt = new DateTime(2025, 2, 19, 12, 0, 0, 0, DateTimeKind.Utc),
                            Description = "A summary of my bachelor's thesis on modernizing Vitec Aloc's Booking system by replacing virtual machines with Docker containers, achieving 93.75% less disk usage and improved maintainability.",
                            IsPublished = true,
                            PublishedAt = new DateTime(2025, 2, 19, 12, 0, 0, 0, DateTimeKind.Utc),
                            Slug = "bachelors-thesis-scaling-resource-optimization-docker",
                            Tags = "Docker,.NET,Blazor,Clean Architecture,Bachelor Thesis",
                            Title = "My Bachelor's Thesis: Scaling and Resource-Optimization with Docker",
                            UpdatedAt = new DateTime(2025, 2, 19, 12, 0, 0, 0, DateTimeKind.Utc)
                        });
                });
#pragma warning restore 612, 618
        }
    }
}
